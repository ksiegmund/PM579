---
title: "Feature Filtering"
author: "ks"
date: "`r Sys.Date()`"
output: html_document
---

# {.tabset}

## Prostate Cancer Data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(limma)
library(genefilter)
library(matrixStats)
library(tidyverse)
library(qvalue)
```

We're going to look for genes that respond to hormone treatment in the CBP and non-specific control knockdown experiments. 

```{r jbcdat}
load(file = c("../data/JBC 2012/jbcdat.rda"))
geoid <- rownames(jbcdat$targets)[jbcdat$targets$treatment != "sip300"]
subsetEobj <- function(dataobj,geonames){
  subobj <-NULL
     subobj$targets <- dataobj$targets[geonames,]
     subobj$E       <- dataobj$E[,geonames]
     subobj$genes   <- dataobj$genes
     subobj
}
jbcsst <- subsetEobj(jbcdat,geoid)

design <- model.matrix(~factor(jbcsst$targets$hour)+factor(jbcsst$targets$treatment))
fit  <- limma::lmFit(jbcsst$E,design)
efit <- limma::eBayes(fit)
```

### Possible Filters

We do not expect all genes on the array to be expressed in any single tissue (e.g. Prostate cells). A multiplicity correction for all features on the array would penalize our results unnecessarily. So, we want to filter the features in a way to reduce the number of tests without invalidating the pvalues. Overall variance is one such filter. Intensity (mean, upper quartile) is another. Because both statistics are independent to the groups we are stuying we can filter out the genes with low variability or low intensity that are unlikely candidates for differential expression without invalidating the pvalues. 

Create a dataset of the t statistics and two possible filters, here mad and 75th percentile of intensity. Why 75th percentile?
```{r allfilters}

df <- cbind.data.frame( 
            abs.modt=abs(efit$t[,2]),
            dm = efit$coefficients[,2],
            modtp = efit$p.value[,2],
            mads = matrixStats::rowMads(jbcsst$E),
            inty=apply(jbcsst$E,1,function(x) quantile(x,0.75))
      )
head(df)
```

Create a function to count the number of rejected tests for all possible cutoffs of a second variable. The second variable ('method') will be either the BH-adjusted pvalue or the q-value.
```{r smryresults}
nrjct <- function(fname=c("none"),plist,method=c("BH","q")){
      if(method=="BH")  qv = p.adjust(plist)
      if(method=="q")   qv = qvalue(plist)$qvalue
      pv <- cbind.data.frame(pv = plist,
                            qv=qv) %>%
            arrange(qv)
      nr <- pv %>%
        mutate(n = 1,
               filt=fname) %>%
        mutate(n_t  = cumsum(n))   %>%
        group_by(qv)  %>%
          slice(which.max(n_t)) %>%
        filter(qv < 0.20)
      return(nr)
}
```


## moderated t-tests; mad-filter

Let's use MAD as a filter. We'll find the mode of the distribution, and remove all features with MAD less than the mode.

```{r madfilter}
mads <- matrixStats::rowMads(jbcsst$E)
sh <- shorth(mads)
mean(mads < sh)
```

This shows 37\% of the features are below the mode in our dataset. 

```{r histsds}
hist(mads,breaks=50,col="mistyrose",xlab="median absolute deviation")
abline(v=sh,col="blue",lwd=3,lty=2)
```

```{r sdsfilt}
f1 <- which(df$mads > sh)
rn1out <- rbind.data.frame(
            nrjct("none",df$modtp,method = "BH"),
            nrjct("var-filter",df$modtp[f1],method = "BH"),
            nrjct("random",df$modtp[sample(1:length(df$modtp),length(f1))],method = "BH"))
ggplot(rn1out,aes(qv,n_t,group=filt,color=filt))+
  geom_step() + xlab("Estimated FDR")  +
    ylab("# tests rejected")
```

If we filter the 37\% of features with low median absolute deviation, we reject more tests for the same expected FDR (blue vs red line). If we filter a random 37\% of the features, we reject far fewer tests (green line). When we filter genes at random, we've lost true positives and reject fewer tests even though the multiplicity correction is for fewer tests. Using a filter on the variance, we enrich the short list for differentially expressed genes, and reject more tests overall. We saw this previously when we discussed filtering for dimension reduction (week 2).

Here are the volcano plots before and after filtering.
```{r volcano}
par(mfrow=c(1,2))
signif <- -log10(df$modtp)
smoothScatter(df$dm,signif,xlab="Log2 Fold Change",ylab="-log10 (P-value)",
     pch=".",xlim=c(-3,4.5),ylim=c(0,18))

f1 <- which(df$mads > sh)
signif <- -log10(df$modtp[f1])
smoothScatter(df$dm[f1],signif,xlab="Log2 Fold Change",ylab="-log10 (P-value)",
     pch=".",xlim=c(-3,4.5),ylim=c(0,18))
```

It's hard to notice the larger white space around 0 in the figure on the right. It would be more obvious if we had run ordinary t-tests instead of moderated t-tests. Still, removing genes with low variance has the effect of removing genes with low fold-change.

## abs(t) vs filter

Plot the absolute t-statistic vs the filter statistic, ranked from low to high. Two filters are considered, median absolute deviation (mad) and the 75th percentile of the (log2) intensity. The mad filters based on variability and the 75th percentile of the intensity is ranking genes on the minimum expression value in top 25\% of the samples. In the figures below, red points indicate the 95th percentile of the absolute t-values grouped in bins along the x-axis.

```{r abstf1}
idxmads <- df %>% 
          arrange(mads) %>%
          mutate(idx = 1:length(df$abs.modt)) %>%
          mutate(gpidx = ntile(idx, 20))

gett95ub <- function(data) {
            data %>%
            group_by(gpidx) %>%
            summarize(ub95 = quantile(abs.modt,0.95),
                      midx = mean(idx))
            }

qnmad <- gett95ub(idxmads)
```

```{r abst.inty}
# do for intensity
idxinty <- df %>% 
          arrange(inty) %>%
          mutate(idx = 1:length(df$abs.modt)) %>%
          mutate(gpidx = ntile(idx, 20))
qninty <- gett95ub(idxinty)

par(mfrow=c(1,2))
smoothScatter(idxmads$idx,idxmads$abs.modt,xlab="Sorted by MAD",ylab="|T|")
points(qnmad$midx,qnmad$ub95,pch=16,col=2)
smoothScatter(idxinty$idx,idxinty$abs.modt,
              xlab="Sorted by Int Q(75)",ylab="|T|")
points(qninty$midx,qninty$ub95,pch=16,col=2)
```

Both of these plots show the majority of large t statistics occur in the right half of the figures. This suggests there will be a huge benefit of filtering low values so there is less of a penalty for the multiplicity correction.  

## Rej tests vs FDR

Now let's compute the number of rejected tests after filtering different fractions of the feature list. 
The first filter we'll call is the median absolute deviation.

```{r madfilt}
f3 <- which(df$mads > quantile(df$mads,0.3))
f5 <- which(df$mads > quantile(df$mads,0.5))
f7 <- which(df$mads > quantile(df$mads,0.7))
f9 <- which(df$mads > quantile(df$mads,0.9))
madout <- rbind.data.frame(
            nrjct("0%",df$modtp,method = "BH"),
            nrjct("30%",df$modtp[f3],method = "BH"),
            nrjct("50%",df$modtp[f5],method = "BH"),
            nrjct("70%",df$modtp[f7],method = "BH"),
            nrjct("90%",df$modtp[f9],method = "BH"))

#ggplot(madout,aes(qv,n_t,group=filt,color=filt))+
#  geom_step()
```

The second filter we'll consider is the intensity of the 75th percentile.

```{r intyfilt}
f3 <- which(df$inty > quantile(df$inty,0.3))
f5 <- which(df$inty > quantile(df$inty,0.5))
f7 <- which(df$inty > quantile(df$inty,0.7))
f9 <- which(df$inty > quantile(df$inty,0.9))
intyout <- rbind.data.frame(
            nrjct("0%",df$modtp,method = "BH"),
            nrjct("30%",df$modtp[f3],method = "BH"),
            nrjct("50%",df$modtp[f5],method = "BH"),
            nrjct("70%",df$modtp[f7],method = "BH"),
            nrjct("90%",df$modtp[f9],method = "BH"))

#ggplot(intyout,aes(qv,n_t,group=filt,color=filt))+
#  geom_step()
```

Now let's compare the results from these 2 different filters.
```{r comparefilts}
madoutn <- cbind.data.frame( madout,
                              fname = "mad")

intyoutn <- cbind.data.frame(intyout ,
                              fname="intensity")

pnldat <- rbind.data.frame(madoutn,
                           intyoutn)
pnldat$fname_f <- factor(pnldat$fname,levels=c("mad","intensity"))
ggplot(pnldat,aes(qv,n_t,group=filt,color=filt))+
    geom_step() + facet_grid(.~fname_f) + xlab("Estimated FDR")  +
    ylab("# tests rejected")
```

They perform pretty similarly until we get to filtering 70% of the features. There MAD filter shows a higher number of rejected tests for the same estimated FDR. If we filter 90\% of features the MAD rejects even more tests, while now the intensity method has started to filter out some of the high T-statistics.

HOWEVER! If we want to interpret the p-values (qvalues) at the end of the study, we cannot try different cutoffs and pick the best one. That comparison would be an improper use of the class labels to tune the final results. But, this comparison supports the results from the PNAS paper that a measure of variability gives a better filter criterion than of a measure of intensity. We should confirm this again in another data set.

The exact benefits of the filtering will vary from dataset to dataset, but a multiplicity correction for tests that are always going to be null (here, unexpressed genes), will only hurt our ability to find true results.

```{r sessioninfo}
sessionInfo()
```
