---
title: "Visualize Single-Cell RNA seq"
author: "ks"
date: "2023-05-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (!require(scuttle)) BiocManager::install("scuttle")
if (!require(scater)) BiocManager::install("scater")
if (!require(OSCA.intro)) BiocManager::install("OSCA.intro")
require(data.table)
require(bluster)
```


package citation: https://academic.oup.com/bioinformatics/article/33/8/1179/2907823

We're going to use data from https://pubmed.ncbi.nlm.nih.gov/35538548/ that is
accessed here:
https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE200997

```{r read-data, cache = TRUE}
start_time <- Sys.time()
sparse.mat <- readSparseCounts("~kims/Google Drive/My Drive/Teaching/PM579/2023 working folder/data/GSE200997_GEO_processed_CRC_10X_raw_UMI_count_matrix.csv.gz",
                               sep=",", quote = "\"")
end_time <- Sys.time()
end_time - start_time

# ~68 Mb    
dim(sparse.mat)
```



```{r read-annot}
coldata <- read.delim("~kims/Google Drive/My Drive/Teaching/PM579/2023 working folder/data/GSE200997_GEO_processed_CRC_10X_cell_annotation.csv.gz", sep = ",", row.names = 1, check.names=FALSE)

identical(rownames(coldata),colnames(sparse.mat))
```

The authors report using counts from 49859 cells.
```{r sce, cache = TRUE}
sce <- SingleCellExperiment( assays = list(counts = sparse.mat),
                            colData = coldata)
sce
```

How many tissue samples were analyzed?

```{r char-samples}
length(unique(colData(sce)$samples))
table(colData(sce)$samples)
```
Here is the distribution of samples by tissue type, location in the colon and MSI status.
```{r sample-char}
# Selecting the relevant columns, and keeping unique records
schar <- unique(as.data.table(colData(sce))[, list(samples,Condition, Location, MSI_Status)])
with(schar, table(Condition, Location, MSI_Status))
```

Let's add some cell quality control metrics. 
```{r cellqc}
sce <- scuttle::addPerCellQC(sce)
colData(sce)
```

All cells with fewer than 200 genes detected were omitted from the paper. The variable detected gives us that count.
```{r qcfilter}
summary(colData(sce)$detected)
```
Yes, this criteria matches what was reported in the paper.

They also filtered genes that were not detected in at least 3 samples.
```{r}
rpos <- rowSums(counts(sce) != 0)
f1 <- rpos > 2
table(f1)
```

Now we have 347 genes to filter out.
```{r}
sce <- sce[f1,]
dim(sce)
```


## Feature selection

We select the top variable genes for PCA analysis. Variation is measured within samples. Looking at the details of the method, variation is modeled as a function of the mean using a smoothed (lowess) curve. Then biological variation is the difference between the observed variation and that predicted from the curve (residual). They don't mention the number, but in Seurat (the software they use for analysis), the default is 2000, so let's use that number here.

```{r pick-hvg}
sce <- scater::logNormCounts(sce)

dec <- scran::modelGeneVar(sce, block=sce$samples)
hvg <- scran::getTopHVGs(dec, n=2000)
length(hvg)
```

```{r batch-correction}
set.seed(1234)
sce <- correctExperiments(sce, batch=sce$samples, 
    subset.row=hvg, correct.all=TRUE)
```

Why do we skip PCA on batch corrected data? I don't think they did this in the paper.  The next step took forever with 1728 genes. Let's do the PCA now

first look at the dimred methods in sce. (can I find: use.dimred='corrected'??)

```{r PCA}
set.seed(5648)
start_time <- Sys.time()
sce <- runPCA(sce, ncomponents=30, subset_row=hvg)

end_time <- Sys.time()
end_time - start_time
dim(reducedDim(sce, "PCA"))
```



```{r clustering}
colLabels(sce) <- clusterCells(sce, use.dimred='corrected')
```


```{r vizclusters}
sce <- runTSNE(sce, dimred = 'corrected')
gridExtra::grid.arrange(
    plotTSNE(sce, colour_by="label"),
    plotTSNE(sce, colour_by="samples"),
    ncol=2
)
```


```{r markers}
# Marker detection, blocking on the individual of origin.
markers <- findMarkers(sce, test.type="wilcox", direction="up", lfc=1)
```

now go back and do PCA on hvg just like in paper.

Find Kelly's hvg selection procedure.

OLD STUFF I tested

```{r PCA}
set.seed(5648)
start_time <- Sys.time()
sce <- runPCA(sce, ncomponents=30, subset_row=hvg)

end_time <- Sys.time()
end_time - start_time
dim(reducedDim(sce, "PCA"))
```

```{r clustering}
colLabels(sce) <- scran::clusterCells(sce, use.dimred='PCA',
    BLUSPARAM=NNGraphParam(cluster.fun="louvain"))    
```


```{r viz}
sce <- runUMAP(sce, dimred = 'PCA')
plotUMAP(sce, colour_by="label")
```



```{r tsne}
start_time <- Sys.time()
sce <- scater::runTSNE(sce, perplexity = 0.1)
end_time <- Sys.time()
end_time - start_time

head(reducedDim(sce, "TSNE"))
```


```{r umap}
start_time <- Sys.time()
u <- scater::calculateUMAP(logcounts(sce), n_neighbors = 2)
end_time <- Sys.time()
end_time - start_time

reducedDim(sce, "UMAP") <- u
reducedDims(sce) # Now stored in the object.
```

```{r}
head(reducedDim(sce, "UMAP"))
```

Two methods for size factors. This first one took a LONG time to run. It pools counts from many cells and then deconvolutes them for normalization. The second one, that just scales by total read count, took just seconds.
```{r sf}
start_time <- Sys.time()
sce <- scran::computeSumFactors(sce)
end_time <- Sys.time()
end_time - start_time

summary(sizeFactors(sce))
```

```{r scsf}
start_time <- Sys.time()
sizeFactors(sce) <- scater::librarySizeFactors(sce)
end_time <- Sys.time()
end_time - start_time

summary(sizeFactors(sce))
```

