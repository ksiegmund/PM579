---
title: "Classification in High-Dimensions"
author: "Kim Siegmund"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# {.tabset}

## Background

```{r fncs}
if (!require(MASS)) install.packages("MASS") # package needed to generate correlated precictors
if (!require(glmnet)) install.packages("glmnet") # package needed to perform ridge/lasso/elastic net regression

library(MASS)
library(glmnet)

# function to turn log odds into probability
expit <- function(x){  
  ex <- exp(x)
  p  <- ex/(1+ex)
  p
}
```

Let's simulate an outcome variable with 2 classes, and use the logistic (binomial) model to predict class. This time I'm going to simulate predictors that are correlated. 

Turn example 3 into a classification exercise, in high dimensions:
https://www4.stat.ncsu.edu/~post/josh/LASSO_Ridge_Elastic_Net_-_Examples.html#example-3

Example 3 uses a correlation matrix that is strongest between neighbors, and the correlation decays with # neighbors apart.

**Ex 3.  Correlation  matrix**
```{r cormatrix}
p<-50
CovMatrix <- outer(1:p, 1:p, function(x,y) {.7^abs(x-y)})
lattice::levelplot(as.matrix(CovMatrix))
```

**Blk diag Corr matrix**

I'm going to use a block diagonal matrix instead. I'll show the correlation structure for the first 100 variables, but we'll model 1000. 
```{r blkdiag}
blocksize <- 5
CovMatrix <- outer(rep(0.9,blocksize), rep(0.9,blocksize)) + (1-0.9^2)*diag(blocksize)
CovMatrix
IdpMatrix <- diag(70)
BlockMtrx <- Matrix::bdiag(CovMatrix,CovMatrix,CovMatrix,CovMatrix,CovMatrix,CovMatrix,IdpMatrix)

lattice::levelplot(as.matrix(BlockMtrx))
```

## Correlated X

**Model information**

n=500
p=1000

The first 15 model parameters are informative of class.
Model Parameters = 2-4's, 3-2's, 10-1's, 985- 0's  
We'll specify an exchangeable correlation structure: 6 blocks of 5 features with cor=0.81 and 970 uncorrelated features.

**Generate data**

First, create the block diagonal covariance matrix for the predictors X.
```{r CovMatrix}
set.seed(22)
n <- 500    # Number of observations
p <- 1000     # Number of predictors included in model
blocksize <- 5
CovMatrix <- outer(rep(0.9,blocksize), rep(0.9,blocksize)) + (1-0.9^2)*diag(blocksize)
BlkMtrx <- Matrix::bdiag(CovMatrix,CovMatrix,CovMatrix,CovMatrix,CovMatrix,CovMatrix)
```

Now simulate X, with the first 30 variables coming from a block diagonal matrix and the last 970 as iid random normal deviates.
```{r simdat}
# make X matrix
x30 <- mvrnorm(n=n, rep(0,30), BlkMtrx) # n observations from the correlated X variables
xp <- replicate(n=(p-30),rnorm(n,0,1)) # n observations from the independent X variables
x <- cbind(x30,xp)  #  X matrix
```


Estimate outcome from the logistic regression model. This will be the logit p, with p the binomial probability of y=1.
```{r logitp}
logitp <-  4 * apply(x[, 1:2], 1, sum) + 
  2 * apply(x[, 3:5], 1, sum) +
  apply(x[, 6:15], 1, sum)  # the first 15 variables are informative of class
```

Randomly sample outcome (y) from a binomial distribution with probability p. This is a vector of length 500.
```{r simy}
prb <- expit(logitp)     # probabilty y=1  
y <- rbinom(length(prb),size=1,prb)  # y is the number of 'successes' in size = 1 trials. class 1 with probability prb and class 0 otherwise
table(prb>0.5,y)        # table of probability vs class (1/0)
```

Split data into train and test sets
```{r train-test}
train_rows <- sample(1:n, .66*n)
x.train <- x[train_rows, ]
x.test <- x[-train_rows, ]

y.train <- y[train_rows]
y.test <- y[-train_rows]
```


**fit models**

```{r fit-models}
set.seed(34)
list.of.fits <- list()
for (i in 0:10){
  fit.name <- paste0("alpha", i/10)
  
  list.of.fits[[fit.name]] <-
    cv.glmnet(x.train, y.train, type.measure = "deviance", alpha = i/10,
              family = "binomial")
}

results <- data.frame()
for (i in 0:10){
  fit.name <- paste0("alpha", i/10)
  
  predicted <-
    predict(list.of.fits[[fit.name]] ,
           s = list.of.fits[[fit.name]]$lambda.1se, newx = x.test)
  
  confumat <- table(y.test,predicted>0)
  error.rate <- (confumat[1,2] + confumat[2,1])/sum(confumat)
  
  temp  <- data.frame(alpha = i/10, error.rate =  error.rate, fit.name = fit.name)
  results <- rbind(results,temp)
}
results
```



```{r fits}
fit.lasso <- glmnet(x.train, y.train, family="binomial", alpha=1)
fit.ridge <- glmnet(x.train, y.train, family="binomial", alpha=0)
fit.elnet <- glmnet(x.train, y.train, family="binomial", alpha=.5)


# 10-fold Cross validation for each alpha = 0, 0.1, ... , 0.9, 1.0
fit.lasso.cv <- cv.glmnet(x.train, y.train, type.measure="deviance",
                          alpha=1, 
                          family="binomial")
fit.ridge.cv <- cv.glmnet(x.train, y.train, type.measure="deviance",
                          alpha=0,
                          family="binomial")
fit.elnet.cv <- cv.glmnet(x.train, y.train, type.measure="deviance",
                          alpha=.5,
                          family="binomial")
```

**plot solution paths and cv mse**

```{r plots}
# Plot solution paths:
par(mfrow=c(3,2))
# For plotting options, type '?plot.glmnet' in R console
plot(fit.lasso, xvar="lambda")
plot(fit.lasso.cv, main="LASSO")

plot(fit.ridge, xvar="lambda")
plot(fit.ridge.cv, main="Ridge")

plot(fit.elnet, xvar="lambda")
plot(fit.elnet.cv, main="Elastic Net")
```

## Indep X

**Generate data**

```{r simindepX}
set.seed(24)
n <- 500    # Number of observations
p <- 1000     # Number of predictors included in model
x <- replicate(n=p,rnorm(n,0,1)) # X matrix
logitp <-  4 * apply(x[, 1:2], 1, sum) + 
  2 * apply(x[, 3:5], 1, sum) +
  apply(x[, 6:15], 1, sum)  # the first 15 variables are informative of class

prb <- expit(logitp)     # probabilty y=1  
y <- rbinom(length(prb),1,prb)  # y is class 1 with probability prb and class 0 otherwise
table(prb>0.5,y)        # table of probability vs class (1/0)

# Split data into train and test sets
train_rows <- sample(1:n, .66*n)
x.train <- x[train_rows, ]
x.test <- x[-train_rows, ]

y.train <- y[train_rows]
y.test <- y[-train_rows]
```


**fit models**

```{r fit-models-indepX}
set.seed(34)
list.of.fits <- list()
for (i in 0:10){
  fit.name <- paste0("alpha", i/10)
  
  list.of.fits[[fit.name]] <-
    cv.glmnet(x.train, y.train, type.measure = "deviance", alpha = i/10,
              family = "binomial")
}

results <- data.frame()
for (i in 0:10){
  fit.name <- paste0("alpha", i/10)
  
  predicted <-
    predict(list.of.fits[[fit.name]] ,
           s = list.of.fits[[fit.name]]$lambda.1se, newx = x.test)
  
  confumat <- table(y.test,predicted>0)
  error.rate <- (confumat[1,2] + confumat[2,1])/sum(confumat)
  
  temp  <- data.frame(alpha = i/10, error.rate =  error.rate, fit.name = fit.name)
  results <- rbind(results,temp)
}
results
```

## Session Information
```{r sI}
sessionInfo()
```
